{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d67e1c1f-d78d-40ab-b07b-f43e55b10381",
   "metadata": {},
   "source": [
    "# Inference tutorial\n",
    "\n",
    "This notebook will go over:\n",
    "\n",
    "- how to simulate an offline training dataset.\n",
    "- how to train the approximate likelihood and posterior\n",
    "- how to run the HMC update procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f454d9da-b09d-4b53-9a22-df31cfacdffc",
   "metadata": {},
   "source": [
    "We begin by importing a few packages we will need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631ef988-0ae3-43f4-ad9c-c705887ed7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import logging\n",
    "import torch as t\n",
    "import numpy as np\n",
    "\n",
    "from textwrap import wrap\n",
    "\n",
    "import spt\n",
    "import spt.config as cfg\n",
    "import spt.inference.san as san\n",
    "import spt.modelling.simulation as sim\n",
    "\n",
    "from spt.types import Tensor\n",
    "from spt.visualisation import plot_corner, plot_posteriors, ppplot\n",
    "from spt.load_photometry import get_norm_theta, get_denorm_theta, get_denorm_theta_t, load_simulated_data, load_real_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4bc873-e6c2-468b-a3fb-477cd887bb63",
   "metadata": {},
   "source": [
    "Since we're working inside a notebook, we'll change directory to the root of the SPItorch project so that we'll be able to access the example data and datasets in a portable way. We'll also take care of other setup stuff, which will become relevant in later tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3106c376-59a6-48e1-a240-1f9398743afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "try: # One-time setup\n",
    "    assert(_SETUP)\n",
    "except NameError:\n",
    "    os.chdir(os.path.split(spt.__path__[0])[0])\n",
    "    dtype = t.float32\n",
    "    device = t.device(\"cuda\") if t.cuda.is_available() else t.device(\"cpu\")\n",
    "    if device == t.device(\"cuda\"):\n",
    "        print(f'Using GPU for training')\n",
    "        !nvidia-smi -L\n",
    "    else:\n",
    "        print(\"CUDA is unavailable; training on CPU.\")\n",
    "        \n",
    "    def dc(x: Tensor) -> Tensor:\n",
    "        return x.detach().cpu()\n",
    "    def dcn(x: Tensor) -> np.ndarray:\n",
    "        return x.detach().cpu().numpy()\n",
    "        \n",
    "    _SETUP = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4a0b92-8ed9-4a45-b0ad-0cd0cfc11128",
   "metadata": {},
   "source": [
    "## Loading Configurations\n",
    "\n",
    "We begin by loading configurations from the configuration file (`./spt/config.py`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6733a802-f386-4ba1-8709-93d625ef078c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ip = cfg.InferenceParams()\n",
    "fp = cfg.ForwardModelParams()\n",
    "dt = get_denorm_theta(fp)\n",
    "dtt = get_denorm_theta_t(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e140b6-ce40-4da3-b420-95f7fc92e6ae",
   "metadata": {},
   "source": [
    "We can inspect the parameters for our Prospector forward model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4340f195-2cab-490b-9861-43bfd1396049",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69393164-368a-4d18-854f-91d2322a77bf",
   "metadata": {},
   "source": [
    "## Simulating a Dataset\n",
    "\n",
    "To simulate a dataset using the Prospector forward model, configure the `SamplingParams` in the configuration file, and provide these to the entrypoint of the `simulation` module (alias `sim`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e46a5f8-2e0d-4607-ba16-a6e052075591",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = cfg.SamplingParams()\n",
    "print(sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b119eb87-6c4a-4c0d-b2ac-9295eb9d4732",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim.main(sp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fec2c51-8eb9-450b-ab05-91942c4c5fc8",
   "metadata": {},
   "source": [
    "## Maximum Likelihood Training of SAN Posterior\n",
    "\n",
    "We will now use the simulated dataset generated above to train a neural density estimator (here we use the 'v2' variant of our _Sequential Autoregressive Network_) since it performs better.\n",
    "\n",
    "<img src=\"https://share.maximerobeyns.com/sanv2.svg\" max-width=\"800px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d116e044-3370-4c78-827c-8ab8db56651e",
   "metadata": {},
   "source": [
    "Here, we load the SANv2 parameters from the configuration, and initialise a the neural density estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea76bf72-b3f3-463c-851d-853ab04bb319",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp = cfg.SANv2Params()\n",
    "Q = san.SANv2(mp)\n",
    "logging.info(f'Initialised {Q}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab89956a-2ed0-40ab-be80-df9548799be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is not strictly necessary, but is useful for portability\n",
    "Q.device = device\n",
    "Q = Q.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91784ba-f6de-4adb-b7fe-4f69f9620562",
   "metadata": {},
   "source": [
    "### Load the training data\n",
    "\n",
    "Before we can proceed with training, we must load up the training dataset that we just simulated above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685bc19c-6976-450d-9b2d-9254bcc2ac74",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = load_simulated_data(\n",
    "    path=ip.dataset_loc,\n",
    "    split_ratio=ip.split_ratio,\n",
    "    batch_size=Q.params.batch_size,\n",
    "    phot_transforms=[np.log, t.from_numpy],\n",
    "    theta_transforms=[get_norm_theta(fp)],\n",
    ")\n",
    "logging.info('Created data loaders')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919757ca-817a-441c-9f92-5bef8121034e",
   "metadata": {},
   "source": [
    "We will also create some convenience methods for later while we're at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a006bdd-511f-4c61-9265-20c3fad1fb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convenience methods\n",
    "tds = test_loader.dataset\n",
    "def new_sim_sample() -> tuple[int, int, tuple[Tensor, Tensor]]:\n",
    "    \"\"\"Returns index in the test loader, index in the simulated dataset and (x, y) pair\"\"\"\n",
    "    ds_idx = random.randint(0, len(tds)-1) # test loader index\n",
    "    xys = tds[ds_idx]\n",
    "    idx = tds.indices[ds_idx]\n",
    "    sim_xs = t.from_numpy(xys[0]) if isinstance(xys[0], np.ndarray) else xys[0]\n",
    "    sim_ys = t.from_numpy(xys[1]) if isinstance(xys[1], np.ndarray) else xys[1]\n",
    "    return ds_idx, idx, (sim_xs.to(device, dtype), sim_ys.to(device, dtype))\n",
    "\n",
    "def sim_sample_at(ds_idx: int) -> tuple[int, tuple[Tensor, Tensor]]:\n",
    "    xys = tds[ds_idx]\n",
    "    idx = tds.indices[ds_idx]\n",
    "    sim_xs = t.from_numpy(xys[0]) if isinstance(xys[0], np.ndarray) else xys[0]\n",
    "    sim_ys = t.from_numpy(xys[1]) if isinstance(xys[1], np.ndarray) else xys[1]\n",
    "    return idx, (sim_xs.to(device, dtype), sim_ys.to(device, dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3700bfc-7cec-44a2-826c-de5ac9197ce8",
   "metadata": {},
   "source": [
    "### Run the Training Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebba896b-6d24-4524-894a-af55ebdde7d4",
   "metadata": {},
   "source": [
    "We can now proceed to call the training method as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19e5e21-531e-43be-9b36-65ee01506406",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q.offline_train(train_loader, ip)\n",
    "logging.info('ML training of approximate posterior complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9da816-ac8e-4e4f-a9ed-a91baeced5d2",
   "metadata": {},
   "source": [
    "As a quick evaluation to see whether the trained model is any good, we can visualise some posteriors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d12b9e-4bd5-4f51-8260-9ebb8ec38882",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q.eval()  # put the model in 'evaluation mode'\n",
    "ds_idx, idx, (sim_xs, sim_ys) = new_sim_sample()  # pick a random sample\n",
    "\n",
    "sim_xs, _ = Q.preprocess(sim_xs, sim_ys)\n",
    "post_samples = dcn(Q.sample(sim_xs, 10000)).squeeze()\n",
    "\n",
    "plot_corner(samples=post_samples,\n",
    "            true_params=dcn(sim_ys).squeeze(),\n",
    "            lims=fp.free_param_lims(normalised=True),\n",
    "            title=f'$Q(\\\\theta \\\\vert x_{{{idx:,}}})$ simulated test point posterior',\n",
    "            description=\"\\n\".join(wrap(str(Q), 160)))\n",
    "t.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1482983-af4b-4b3f-a6fc-89c8a6d44e59",
   "metadata": {},
   "source": [
    "We can also plot samples from the approximate posteior against the 'ground truth' values for a sample of points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0e943f-75dd-4853-9c93-4d6e0ddea31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n, samples = 1000, 1000\n",
    "test_xs, test_ys = test_loader.dataset[:n]\n",
    "test_xs = t.from_numpy(test_xs) if isinstance(test_xs, np.ndarray) else test_xs\n",
    "test_ys = t.from_numpy(test_ys) if isinstance(test_ys, np.ndarray) else test_ys\n",
    "test_xs, test_ys = Q.preprocess(test_xs, test_ys)\n",
    "\n",
    "with t.inference_mode():\n",
    "    Q.eval()\n",
    "    _ = Q(test_xs, True)\n",
    "    test_y_hat = Q.likelihood._gmm_from_params(Q.last_params).sample((samples,)).reshape(-1, 6)\n",
    "    plot_ys = test_ys[None, :].expand((samples, n, 6)).reshape(-1, 6)\n",
    "\n",
    "plot_posteriors(test_y_hat.cpu().numpy(), plot_ys.cpu().numpy(),\n",
    "                labels=fp.ordered_free_params, \n",
    "                title='Posterior samples for simulated test points', \n",
    "                description=f'{samples} samples drawn for {n} test data points, plotted against the true values.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdd506e-6575-410c-ba12-88a4c73ff006",
   "metadata": {},
   "source": [
    "## Maximum Likelihood training of neural likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0b97ec-fb2d-4b0d-92b6-2739eae6727e",
   "metadata": {},
   "source": [
    "We can now repeat a similar procedure to train the neural likelihood. There are a couple of exceptions:\n",
    "- we must remember to swap the dimensions of the inputs and outputs during preprocessing\n",
    "\n",
    "  To help with this, the `san.PModel` (or `san.Pmodelv2` for `SANv2`) implements the required preprocessing steps.\n",
    "  \n",
    "- we can configure the network to be smaller since the likelihood is a simpler distribution to approximate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74d14f4-0fcb-46bf-bfef-991e77b9328f",
   "metadata": {},
   "outputs": [],
   "source": [
    "slp = cfg.SANv2LikelihoodParams()\n",
    "P = san.PModelv2(slp)\n",
    "ip.ident = \"ML_likelihood\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052cf1ff-fd0a-4a0b-8c41-d844b8f6a177",
   "metadata": {},
   "outputs": [],
   "source": [
    "P.offline_train(train_loader, ip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8f1c66-8f11-491e-b281-c100c1de87b8",
   "metadata": {},
   "source": [
    "### Evaluating the Neural Likelihood\n",
    "\n",
    "To check that the training was reasonable successful, we can plot sampled points against the 'ground truths':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6c4f19-06e1-4128-9d99-ed4c104680f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "test_xs, test_ys = test_loader.dataset[:n]\n",
    "test_xs = t.from_numpy(test_xs) if isinstance(test_xs, np.ndarray) else test_xs\n",
    "test_ys = t.from_numpy(test_ys) if isinstance(test_ys, np.ndarray) else test_ys\n",
    "test_xs, test_ys = P.preprocess(test_xs, test_ys)\n",
    "    \n",
    "with t.inference_mode():\n",
    "    _ = P(test_xs, True)\n",
    "    test_y_hat = P.likelihood._gmm_from_params(P.last_params).mean\n",
    "    plot_ys = test_ys\n",
    "\n",
    "test_y_hat, plot_ys = map(dcn, (test_y_hat, plot_ys))\n",
    "plot_posteriors(test_y_hat, plot_ys,\n",
    "                labels=list(range(mp.cond_dim)), lims=False,\n",
    "                title='$P_{w}(x \\\\vert \\\\theta)$ for simulated test points', \n",
    "                description=f'Expected (normalised) flux values for {n} test data points, plotted against the true values.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1e354d-c6b3-42db-8e3e-d8ffcf38a71e",
   "metadata": {},
   "source": [
    "# HMC Update Procedure\n",
    "\n",
    "Here we run the HMC update procedure on the weights of the approximate posterior, with examples for using the simulated and real data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b631aa1e-c824-4753-90bc-8ef96329985c",
   "metadata": {},
   "source": [
    "## HMC update procedure with real data\n",
    "\n",
    "For real surveys, we will want to run the HMC update procedure on real data from (a subset of) a survey.\n",
    "\n",
    "We begin by recreating some data loaders, using the HMC update batch size (to allow us to control memory usage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b0e7b2-fada-42a7-9b52-b271cecad1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_train_loader, real_test_loader = load_real_data(\n",
    "    path=ip.catalogue_loc, filters=fp.filters, split_ratio=ip.split_ratio,\n",
    "    batch_size=ip.hmc_update_batch_size, \n",
    "    transforms=[t.from_numpy], x_transforms=[np.log],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3b49c4-af5a-441c-9dee-cdfede9a19c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ip.ident = ip.hmc_update_real_ident\n",
    "Q.hmc_retrain_procedure(real_train_loader, ip, P=P, epochs=ip.hmc_update_real_epochs,\n",
    "                        K=ip.hmc_update_real_K, lr=3e-4, decay=1e-4)\n",
    "logging.info('Updated on real data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78520b07-3ba5-496e-9565-38b9398d78ba",
   "metadata": {},
   "source": [
    "## HMC update procedure on simulated data\n",
    "\n",
    "Alternatively, we can run the update procedure on the simulated data, which will allow us to create evaluation plots against the 'ground truth' values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b63c26-ccd0-4b29-8795-2d4af0452159",
   "metadata": {},
   "outputs": [],
   "source": [
    "hmc_train_loader, hmc_test_loader = load_simulated_data(\n",
    "    path=ip.dataset_loc,\n",
    "    split_ratio=ip.split_ratio,\n",
    "    batch_size=ip.hmc_update_batch_size,\n",
    "    phot_transforms=[t.from_numpy, np.log],\n",
    "    theta_transforms=[get_norm_theta(fp)],\n",
    ")\n",
    "logging.info('Created data loaders')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57d7488-b29b-479d-8013-ed2eb711928d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ip.ident = ip.hmc_update_sim_ident\n",
    "Q.hmc_retrain_procedure(hmc_train_loader, ip, P=P, epochs=ip.hmc_update_sim_epochs, \n",
    "                        K=ip.hmc_update_sim_K, lr=3e-4, decay=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd8dfdd-24ab-49a5-a471-6cb9d528d79a",
   "metadata": {},
   "source": [
    "# Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3cd835-3441-4331-8d7f-479548291bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We re-use this prospector instance to make plotting a little faster later\n",
    "dummy_obs = spt.load_observation()\n",
    "p = spt.Prospector(dummy_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda39cd4-0b1f-4479-b94c-282a47dfd75f",
   "metadata": {},
   "source": [
    "## Setup baseline for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28b97d7-354b-4078-89e2-9cb602f6ebb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ip = cfg.InferenceParams()\n",
    "mp = cfg.SANv2Params()\n",
    "Q_base = san.SANv2(mp)\n",
    "Q_base.offline_train(train_loader, ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febfee93-75f4-4f23-912b-0a1be96173ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_base.device = device\n",
    "Q_base = Q_base.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd61962-8da4-48a0-970f-5047854716c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q.eval()\n",
    "Q_base.eval()\n",
    "\n",
    "ds_idx, idx, (sim_xs, sim_ys) = new_sim_sample()\n",
    "logging.info(f'ds_idx: {ds_idx}')\n",
    "\n",
    "sim_xs, sim_ys = Q.preprocess(sim_xs, sim_ys)\n",
    "start = time.time()\n",
    "with t.inference_mode():\n",
    "    san_mode = Q.mode(sim_xs, 10000)\n",
    "    base_mode = Q_base.mode(sim_xs, 10000)\n",
    "san_mode = dt(san_mode.cpu().squeeze().numpy())\n",
    "base_mode = dt(base_mode.cpu().squeeze().numpy())\n",
    "true_ys = dt(sim_ys.cpu().numpy())\n",
    "\n",
    "phot_obs = np.exp(sim_xs.squeeze().cpu().numpy())\n",
    "obs = spt.load_photometry.sim_observation(fp.filters, phot_obs, index=idx, dset=ip.dataset_loc)\n",
    "p.set_new_obs(obs)\n",
    "\n",
    "p.visualise_model(theta=[san_mode, base_mode, #true_ys\n",
    "                        ], theta_labels=[\"SAN (HMC update)\",  \"Baseline SAN\", #\"True\"\n",
    "                                        ],\n",
    "                  show=True, save=False, title=f'Forward Model Predictions (simulated point, index {ds_idx})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cabb3fb-cd2e-48c5-8487-b2b6af08c55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_ds = spt.load_photometry.InMemoryObsDataset(\n",
    "    ip.dataset_loc,  \n",
    "    phot_transforms=[lambda x: t.from_numpy(np.log(x))],  \n",
    "    theta_transforms=[get_norm_theta(fp)])\n",
    "tmp_xs = sim_ds.get_xs()[:10000]\n",
    "tmp_ys = sim_ds.get_ys()[:10000]\n",
    "sim_xs = t.from_numpy(tmp_xs) if isinstance(tmp_xs, np.ndarray) else tmp_xs\n",
    "sim_ys = t.from_numpy(tmp_ys) if isinstance(tmp_ys, np.ndarray) else tmp_ys\n",
    "\n",
    "sim_xs, sim_ys = Q.preprocess(sim_xs, sim_ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97c9fff-f634-443e-9432-9be49d52473a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with t.inference_mode():\n",
    "    Q.eval()\n",
    "    _ = Q(sim_xs, True)\n",
    "    sim_y_hat = Q.likelihood._gmm_from_params(Q.last_params).sample((100,)).reshape(-1, 6)\n",
    "    plot_ys = sim_ys[None, :].expand((100, 10000, 6)).reshape(-1, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fefd2e-2433-48f4-bee7-2a0b139daa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppplot(sim_y_hat.cpu().numpy(), plot_ys.cpu().numpy(),\n",
    "       labels=fp.ordered_free_params,\n",
    "       title='Probability-Probability plot',\n",
    "       description='\"True\" Simulated CDF vs Prediction CDF')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SPItorch (Python 3.9)",
   "language": "python",
   "name": "agnvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
